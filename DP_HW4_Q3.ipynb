{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joM4PE9FjPBx"
      },
      "source": [
        "## **Load text data:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F7ZM2UG8jzqN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModel, GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrnDoSfBbi-e",
        "outputId": "46136cfe-4978-46bd-e256-bf9a76dd519f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Inputs:\n",
            "['به نام خداوند جان و خرد', 'خداوند نام و خداوند جای', 'خداوند کیوان و گردان سپهر', 'ز نام و نشان و گمان برترست', 'به بینندگان آفریننده را']\n",
            "\n",
            "Outputs:\n",
            "['کزین برتر اندیشه برنگذرد', 'خداوند روزی ده رهنمای', 'فروزنده ماه و ناهید و مهر', 'نگارندهٔ بر شده پیکرست', 'نبینی مرنجان دو بیننده را']\n"
          ]
        }
      ],
      "source": [
        "file_path = 'ferdousi.txt'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "data_inputs = []\n",
        "labels1 = []\n",
        "\n",
        "for i in range(3, len(lines), 2):\n",
        "    data_input = lines[i - 1].strip()\n",
        "    label = lines[i].strip()\n",
        "\n",
        "    data_inputs.append(data_input)\n",
        "    labels1.append(label)\n",
        "\n",
        "print(\"Data Inputs:\")\n",
        "print(data_inputs[:5])\n",
        "\n",
        "print(\"\\nOutputs:\")\n",
        "print(labels1[:5])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_inputs, labels1, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vS5O7lIYbl7"
      },
      "outputs": [],
      "source": [
        "class PoemDataset(Dataset):\n",
        "    def __init__(self, data_inputs, data_outputs, tokenizer, max_length=8):\n",
        "        self.data_inputs = data_inputs\n",
        "        self.data_outputs = data_outputs\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.dataset_size = len(data_inputs)\n",
        "\n",
        "        self.tokenizer.add_special_tokens({'pad_token': '<pad>', \"unk_token\": '<unk>'})\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        idx = idx % self.dataset_size\n",
        "\n",
        "        input_text = self.data_inputs[idx]\n",
        "        output_text = self.data_outputs[idx]\n",
        "\n",
        "        input_ids = self.tokenizer.encode(input_text, max_length=self.max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
        "        labels = self.tokenizer.encode(output_text, max_length=self.max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids.squeeze(),\n",
        "            'labels': labels.squeeze()\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJYYATc4YjzW"
      },
      "outputs": [],
      "source": [
        "model_name = \"HooshvareLab/gpt2-fa\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "data_inputs = X_train\n",
        "data_outputs = y_train\n",
        "dataset = PoemDataset(data_inputs, data_outputs, tokenizer)\n",
        "batch_size = 128\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMRpaaugIcwt",
        "outputId": "20aa9073-12f5-4f4a-f649-7460d27b1d2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/10\n",
            "Batch 10/311, Loss: 8.8037\n",
            "Batch 20/311, Loss: 7.7912\n",
            "Batch 30/311, Loss: 7.3302\n",
            "Batch 40/311, Loss: 7.0510\n",
            "Batch 50/311, Loss: 6.8698\n",
            "Batch 60/311, Loss: 6.7403\n",
            "Batch 70/311, Loss: 6.6477\n",
            "Batch 80/311, Loss: 6.5724\n",
            "Batch 90/311, Loss: 6.5137\n",
            "Batch 100/311, Loss: 6.4696\n",
            "Batch 110/311, Loss: 6.4290\n",
            "Batch 120/311, Loss: 6.3944\n",
            "Batch 130/311, Loss: 6.3646\n",
            "Batch 140/311, Loss: 6.3424\n",
            "Batch 150/311, Loss: 6.3211\n",
            "Batch 160/311, Loss: 6.2991\n",
            "Batch 170/311, Loss: 6.2828\n",
            "Batch 180/311, Loss: 6.2698\n",
            "Batch 190/311, Loss: 6.2558\n",
            "Batch 200/311, Loss: 6.2422\n",
            "Batch 210/311, Loss: 6.2293\n",
            "Batch 220/311, Loss: 6.2169\n",
            "Batch 230/311, Loss: 6.2044\n",
            "Batch 240/311, Loss: 6.1931\n",
            "Batch 250/311, Loss: 6.1823\n",
            "Batch 260/311, Loss: 6.1734\n",
            "Batch 270/311, Loss: 6.1650\n",
            "Batch 280/311, Loss: 6.1554\n",
            "Batch 290/311, Loss: 6.1484\n",
            "Batch 300/311, Loss: 6.1423\n",
            "Batch 310/311, Loss: 6.1350\n",
            "\n",
            "Epoch 2/10\n",
            "Batch 10/311, Loss: 5.8712\n",
            "Batch 20/311, Loss: 5.8375\n",
            "Batch 30/311, Loss: 5.8508\n",
            "Batch 40/311, Loss: 5.8391\n",
            "Batch 50/311, Loss: 5.8385\n",
            "Batch 60/311, Loss: 5.8306\n",
            "Batch 70/311, Loss: 5.8269\n",
            "Batch 80/311, Loss: 5.8246\n",
            "Batch 90/311, Loss: 5.8205\n",
            "Batch 100/311, Loss: 5.8203\n",
            "Batch 110/311, Loss: 5.8169\n",
            "Batch 120/311, Loss: 5.8123\n",
            "Batch 130/311, Loss: 5.8153\n",
            "Batch 140/311, Loss: 5.8153\n",
            "Batch 150/311, Loss: 5.8121\n",
            "Batch 160/311, Loss: 5.8085\n",
            "Batch 170/311, Loss: 5.8078\n",
            "Batch 180/311, Loss: 5.8112\n",
            "Batch 190/311, Loss: 5.8084\n",
            "Batch 200/311, Loss: 5.8080\n",
            "Batch 210/311, Loss: 5.8072\n",
            "Batch 220/311, Loss: 5.8054\n",
            "Batch 230/311, Loss: 5.8043\n",
            "Batch 240/311, Loss: 5.8018\n",
            "Batch 250/311, Loss: 5.7989\n",
            "Batch 260/311, Loss: 5.7967\n",
            "Batch 270/311, Loss: 5.7954\n",
            "Batch 280/311, Loss: 5.7935\n",
            "Batch 290/311, Loss: 5.7923\n",
            "Batch 300/311, Loss: 5.7916\n",
            "Batch 310/311, Loss: 5.7906\n",
            "\n",
            "Epoch 3/10\n",
            "Batch 10/311, Loss: 5.5466\n",
            "Batch 20/311, Loss: 5.5784\n",
            "Batch 30/311, Loss: 5.5853\n",
            "Batch 40/311, Loss: 5.5770\n",
            "Batch 50/311, Loss: 5.5792\n",
            "Batch 60/311, Loss: 5.5891\n",
            "Batch 70/311, Loss: 5.5907\n",
            "Batch 80/311, Loss: 5.5911\n",
            "Batch 90/311, Loss: 5.5940\n",
            "Batch 100/311, Loss: 5.6005\n",
            "Batch 110/311, Loss: 5.6011\n",
            "Batch 120/311, Loss: 5.6035\n",
            "Batch 130/311, Loss: 5.6093\n",
            "Batch 140/311, Loss: 5.6108\n",
            "Batch 150/311, Loss: 5.6093\n",
            "Batch 160/311, Loss: 5.6103\n",
            "Batch 170/311, Loss: 5.6085\n",
            "Batch 180/311, Loss: 5.6078\n",
            "Batch 190/311, Loss: 5.6094\n",
            "Batch 200/311, Loss: 5.6100\n",
            "Batch 210/311, Loss: 5.6090\n",
            "Batch 220/311, Loss: 5.6093\n",
            "Batch 230/311, Loss: 5.6103\n",
            "Batch 240/311, Loss: 5.6096\n",
            "Batch 250/311, Loss: 5.6106\n",
            "Batch 260/311, Loss: 5.6122\n",
            "Batch 270/311, Loss: 5.6112\n",
            "Batch 280/311, Loss: 5.6101\n",
            "Batch 290/311, Loss: 5.6107\n",
            "Batch 300/311, Loss: 5.6119\n",
            "Batch 310/311, Loss: 5.6133\n",
            "\n",
            "Epoch 4/10\n",
            "Batch 10/311, Loss: 5.4071\n",
            "Batch 20/311, Loss: 5.3906\n",
            "Batch 30/311, Loss: 5.3851\n",
            "Batch 40/311, Loss: 5.3825\n",
            "Batch 50/311, Loss: 5.3851\n",
            "Batch 60/311, Loss: 5.3939\n",
            "Batch 70/311, Loss: 5.4016\n",
            "Batch 80/311, Loss: 5.4036\n",
            "Batch 90/311, Loss: 5.4085\n",
            "Batch 100/311, Loss: 5.4080\n",
            "Batch 110/311, Loss: 5.4113\n",
            "Batch 120/311, Loss: 5.4167\n",
            "Batch 130/311, Loss: 5.4172\n",
            "Batch 140/311, Loss: 5.4193\n",
            "Batch 150/311, Loss: 5.4218\n",
            "Batch 160/311, Loss: 5.4251\n",
            "Batch 170/311, Loss: 5.4292\n",
            "Batch 180/311, Loss: 5.4278\n",
            "Batch 190/311, Loss: 5.4288\n",
            "Batch 200/311, Loss: 5.4303\n",
            "Batch 210/311, Loss: 5.4337\n",
            "Batch 220/311, Loss: 5.4350\n",
            "Batch 230/311, Loss: 5.4351\n",
            "Batch 240/311, Loss: 5.4353\n",
            "Batch 250/311, Loss: 5.4368\n",
            "Batch 260/311, Loss: 5.4383\n",
            "Batch 270/311, Loss: 5.4391\n",
            "Batch 280/311, Loss: 5.4399\n",
            "Batch 290/311, Loss: 5.4394\n",
            "Batch 300/311, Loss: 5.4415\n",
            "Batch 310/311, Loss: 5.4410\n",
            "\n",
            "Epoch 5/10\n",
            "Batch 10/311, Loss: 5.1817\n",
            "Batch 20/311, Loss: 5.1566\n",
            "Batch 30/311, Loss: 5.1517\n",
            "Batch 40/311, Loss: 5.1563\n",
            "Batch 50/311, Loss: 5.1601\n",
            "Batch 60/311, Loss: 5.1583\n",
            "Batch 70/311, Loss: 5.1660\n",
            "Batch 80/311, Loss: 5.1719\n",
            "Batch 90/311, Loss: 5.1755\n",
            "Batch 100/311, Loss: 5.1846\n",
            "Batch 110/311, Loss: 5.1898\n",
            "Batch 120/311, Loss: 5.1962\n",
            "Batch 130/311, Loss: 5.1997\n",
            "Batch 140/311, Loss: 5.2048\n",
            "Batch 150/311, Loss: 5.2065\n",
            "Batch 160/311, Loss: 5.2104\n",
            "Batch 170/311, Loss: 5.2148\n",
            "Batch 180/311, Loss: 5.2175\n",
            "Batch 190/311, Loss: 5.2205\n",
            "Batch 200/311, Loss: 5.2205\n",
            "Batch 210/311, Loss: 5.2272\n",
            "Batch 220/311, Loss: 5.2300\n",
            "Batch 230/311, Loss: 5.2315\n",
            "Batch 240/311, Loss: 5.2345\n",
            "Batch 250/311, Loss: 5.2365\n",
            "Batch 260/311, Loss: 5.2404\n",
            "Batch 270/311, Loss: 5.2424\n",
            "Batch 280/311, Loss: 5.2453\n",
            "Batch 290/311, Loss: 5.2484\n",
            "Batch 300/311, Loss: 5.2503\n",
            "Batch 310/311, Loss: 5.2512\n",
            "\n",
            "Epoch 6/10\n",
            "Batch 10/311, Loss: 4.9612\n",
            "Batch 20/311, Loss: 4.9341\n",
            "Batch 30/311, Loss: 4.9181\n",
            "Batch 40/311, Loss: 4.9110\n",
            "Batch 50/311, Loss: 4.9138\n",
            "Batch 60/311, Loss: 4.9287\n",
            "Batch 70/311, Loss: 4.9270\n",
            "Batch 80/311, Loss: 4.9271\n",
            "Batch 90/311, Loss: 4.9337\n",
            "Batch 100/311, Loss: 4.9394\n",
            "Batch 110/311, Loss: 4.9476\n",
            "Batch 120/311, Loss: 4.9554\n",
            "Batch 130/311, Loss: 4.9603\n",
            "Batch 140/311, Loss: 4.9654\n",
            "Batch 150/311, Loss: 4.9719\n",
            "Batch 160/311, Loss: 4.9794\n",
            "Batch 170/311, Loss: 4.9843\n",
            "Batch 180/311, Loss: 4.9902\n",
            "Batch 190/311, Loss: 4.9960\n",
            "Batch 200/311, Loss: 5.0004\n",
            "Batch 210/311, Loss: 5.0032\n",
            "Batch 220/311, Loss: 5.0071\n",
            "Batch 230/311, Loss: 5.0119\n",
            "Batch 240/311, Loss: 5.0171\n",
            "Batch 250/311, Loss: 5.0211\n",
            "Batch 260/311, Loss: 5.0221\n",
            "Batch 270/311, Loss: 5.0253\n",
            "Batch 280/311, Loss: 5.0286\n",
            "Batch 290/311, Loss: 5.0323\n",
            "Batch 300/311, Loss: 5.0355\n",
            "Batch 310/311, Loss: 5.0389\n",
            "\n",
            "Epoch 7/10\n",
            "Batch 10/311, Loss: 4.5863\n",
            "Batch 20/311, Loss: 4.5857\n",
            "Batch 30/311, Loss: 4.5939\n",
            "Batch 40/311, Loss: 4.6049\n",
            "Batch 50/311, Loss: 4.6121\n",
            "Batch 60/311, Loss: 4.6221\n",
            "Batch 70/311, Loss: 4.6356\n",
            "Batch 80/311, Loss: 4.6440\n",
            "Batch 90/311, Loss: 4.6538\n",
            "Batch 100/311, Loss: 4.6665\n",
            "Batch 110/311, Loss: 4.6775\n",
            "Batch 120/311, Loss: 4.6858\n",
            "Batch 130/311, Loss: 4.6938\n",
            "Batch 140/311, Loss: 4.7043\n",
            "Batch 150/311, Loss: 4.7116\n",
            "Batch 160/311, Loss: 4.7184\n",
            "Batch 170/311, Loss: 4.7248\n",
            "Batch 180/311, Loss: 4.7329\n",
            "Batch 190/311, Loss: 4.7380\n",
            "Batch 200/311, Loss: 4.7447\n",
            "Batch 210/311, Loss: 4.7498\n",
            "Batch 220/311, Loss: 4.7590\n",
            "Batch 230/311, Loss: 4.7653\n",
            "Batch 240/311, Loss: 4.7704\n",
            "Batch 250/311, Loss: 4.7740\n",
            "Batch 260/311, Loss: 4.7789\n",
            "Batch 270/311, Loss: 4.7835\n",
            "Batch 280/311, Loss: 4.7871\n",
            "Batch 290/311, Loss: 4.7913\n",
            "Batch 300/311, Loss: 4.7949\n",
            "Batch 310/311, Loss: 4.7976\n",
            "\n",
            "Epoch 8/10\n",
            "Batch 10/311, Loss: 4.3489\n",
            "Batch 20/311, Loss: 4.3541\n",
            "Batch 30/311, Loss: 4.3465\n",
            "Batch 40/311, Loss: 4.3399\n",
            "Batch 50/311, Loss: 4.3345\n",
            "Batch 60/311, Loss: 4.3353\n",
            "Batch 70/311, Loss: 4.3489\n",
            "Batch 80/311, Loss: 4.3649\n",
            "Batch 90/311, Loss: 4.3750\n",
            "Batch 100/311, Loss: 4.3870\n",
            "Batch 110/311, Loss: 4.3963\n",
            "Batch 120/311, Loss: 4.4039\n",
            "Batch 130/311, Loss: 4.4157\n",
            "Batch 140/311, Loss: 4.4234\n",
            "Batch 150/311, Loss: 4.4325\n",
            "Batch 160/311, Loss: 4.4419\n",
            "Batch 170/311, Loss: 4.4480\n",
            "Batch 180/311, Loss: 4.4558\n",
            "Batch 190/311, Loss: 4.4636\n",
            "Batch 200/311, Loss: 4.4713\n",
            "Batch 210/311, Loss: 4.4782\n",
            "Batch 220/311, Loss: 4.4844\n",
            "Batch 230/311, Loss: 4.4905\n",
            "Batch 240/311, Loss: 4.4985\n",
            "Batch 250/311, Loss: 4.5038\n",
            "Batch 260/311, Loss: 4.5087\n",
            "Batch 270/311, Loss: 4.5162\n",
            "Batch 280/311, Loss: 4.5213\n",
            "Batch 290/311, Loss: 4.5262\n",
            "Batch 300/311, Loss: 4.5307\n",
            "Batch 310/311, Loss: 4.5351\n",
            "\n",
            "Epoch 9/10\n",
            "Batch 10/311, Loss: 4.0085\n",
            "Batch 20/311, Loss: 3.9849\n",
            "Batch 30/311, Loss: 3.9891\n",
            "Batch 40/311, Loss: 3.9947\n",
            "Batch 50/311, Loss: 4.0096\n",
            "Batch 60/311, Loss: 4.0203\n",
            "Batch 70/311, Loss: 4.0379\n",
            "Batch 80/311, Loss: 4.0508\n",
            "Batch 90/311, Loss: 4.0643\n",
            "Batch 100/311, Loss: 4.0750\n",
            "Batch 110/311, Loss: 4.0870\n",
            "Batch 120/311, Loss: 4.0968\n",
            "Batch 130/311, Loss: 4.1056\n",
            "Batch 140/311, Loss: 4.1187\n",
            "Batch 150/311, Loss: 4.1291\n",
            "Batch 160/311, Loss: 4.1405\n",
            "Batch 170/311, Loss: 4.1493\n",
            "Batch 180/311, Loss: 4.1554\n",
            "Batch 190/311, Loss: 4.1623\n",
            "Batch 200/311, Loss: 4.1713\n",
            "Batch 210/311, Loss: 4.1819\n",
            "Batch 220/311, Loss: 4.1908\n",
            "Batch 230/311, Loss: 4.1982\n",
            "Batch 240/311, Loss: 4.2050\n",
            "Batch 250/311, Loss: 4.2095\n",
            "Batch 260/311, Loss: 4.2156\n",
            "Batch 270/311, Loss: 4.2201\n",
            "Batch 280/311, Loss: 4.2254\n",
            "Batch 290/311, Loss: 4.2316\n",
            "Batch 300/311, Loss: 4.2362\n",
            "Batch 310/311, Loss: 4.2406\n",
            "\n",
            "Epoch 10/10\n",
            "Batch 10/311, Loss: 3.6576\n",
            "Batch 20/311, Loss: 3.6684\n",
            "Batch 30/311, Loss: 3.6576\n",
            "Batch 40/311, Loss: 3.6758\n",
            "Batch 50/311, Loss: 3.6809\n",
            "Batch 60/311, Loss: 3.6840\n",
            "Batch 70/311, Loss: 3.7079\n",
            "Batch 80/311, Loss: 3.7186\n",
            "Batch 90/311, Loss: 3.7309\n",
            "Batch 100/311, Loss: 3.7421\n",
            "Batch 110/311, Loss: 3.7526\n",
            "Batch 120/311, Loss: 3.7658\n",
            "Batch 130/311, Loss: 3.7766\n",
            "Batch 140/311, Loss: 3.7882\n",
            "Batch 150/311, Loss: 3.7993\n",
            "Batch 160/311, Loss: 3.8101\n",
            "Batch 170/311, Loss: 3.8208\n",
            "Batch 180/311, Loss: 3.8291\n",
            "Batch 190/311, Loss: 3.8405\n",
            "Batch 200/311, Loss: 3.8486\n",
            "Batch 210/311, Loss: 3.8553\n",
            "Batch 220/311, Loss: 3.8640\n",
            "Batch 230/311, Loss: 3.8722\n",
            "Batch 240/311, Loss: 3.8816\n",
            "Batch 250/311, Loss: 3.8885\n",
            "Batch 260/311, Loss: 3.8944\n",
            "Batch 270/311, Loss: 3.9008\n",
            "Batch 280/311, Loss: 3.9077\n",
            "Batch 290/311, Loss: 3.9152\n",
            "Batch 300/311, Loss: 3.9215\n",
            "Batch 310/311, Loss: 3.9279\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('fine_tuned_gpt2_fa/tokenizer_config.json',\n",
              " 'fine_tuned_gpt2_fa/special_tokens_map.json',\n",
              " 'fine_tuned_gpt2_fa/vocab.json',\n",
              " 'fine_tuned_gpt2_fa/merges.txt',\n",
              " 'fine_tuned_gpt2_fa/added_tokens.json')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "    total_loss = 0\n",
        "    total_batches = 0\n",
        "    batsize = len(dataloader)\n",
        "    for batch_index, batch in enumerate(dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_batches += 1\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if (batch_index + 1) % 10 == 0: \n",
        "            average_loss = total_loss / total_batches\n",
        "            print(f\"Batch {batch_index + 1}/{batsize}, Loss: {average_loss:.4f}\")\n",
        "\n",
        "model.save_pretrained(\"fine_tuned_gpt2_fa\")\n",
        "tokenizer.save_pretrained(\"fine_tuned_gpt2_fa\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j5gZrm6JwNV",
        "outputId": "7a3bf6a4-e258-4b51-e4c1-f5fad7db5750"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 56.4307\n"
          ]
        }
      ],
      "source": [
        "test_data_inputs = X_test\n",
        "test_data_outputs = y_test\n",
        "\n",
        "test_dataset = PoemDataset(test_data_inputs, test_data_outputs, tokenizer)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_index, batch in enumerate(test_dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        total_correct += torch.sum(predictions == labels).item()\n",
        "        total_samples += labels.numel()\n",
        "\n",
        "accuracy = total_correct / total_samples\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvMjOimbOzwt",
        "outputId": "131f8a87-75c2-4c12-e566-4655b2f8baf0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:5 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:5 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input:\n",
            " که رستم سلام داد همی\n",
            "Generated Text:\n",
            " به شاد رنج اوی تو و یاد\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:5 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input:\n",
            " همی رفت به انجا ببیند\n",
            "Generated Text:\n",
            " با نهفت و مغز پوشیده بگ\n",
            "\n",
            "input:\n",
            "تو ای مرد سالم ببین این چنین\n",
            "Generated Text:\n",
            "یم تندان زیر دشت\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs_text = [\" که رستم سلام داد همی\", \" همی رفت به انجا ببیند\", \"تو ای مرد سالم ببین این چنین\"]\n",
        "max_length = 13\n",
        "\n",
        "for input_text in inputs_text:\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(input_ids.to(device), max_length=max_length, num_beams=5, temperature=0.7, repetition_penalty=1000000.0)\n",
        "\n",
        "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    print(\"input:\")\n",
        "    print(generated_text[:len(input_text)])\n",
        "    print(\"Generated Text:\")\n",
        "    print(generated_text[len(input_text):])\n",
        "    print()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
